{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705c666d-1828-49d5-9840-70208a7a326b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Integration with Spark\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"Access_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"access_secret_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae4bcab-73fc-4820-886c-cbadf7bcb73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/_delta_log/', name='_delta_log/', size=0, modificationTime=1732678768304),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00000-f7cfb25c-248e-4e7a-8318-878e63e93215.c000.snappy.parquet', name='part-00000-f7cfb25c-248e-4e7a-8318-878e63e93215.c000.snappy.parquet', size=13528168, modificationTime=1732631873000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00001-a3dd5f50-cb6c-45b9-912f-f00038c89bfd.c000.snappy.parquet', name='part-00001-a3dd5f50-cb6c-45b9-912f-f00038c89bfd.c000.snappy.parquet', size=17000175, modificationTime=1732631873000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00002-9397e5ba-9485-49a3-aa1c-ddd2268def29.c000.snappy.parquet', name='part-00002-9397e5ba-9485-49a3-aa1c-ddd2268def29.c000.snappy.parquet', size=17090344, modificationTime=1732631873000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00003-0365f86b-c7bb-4b19-9577-9035f9483b6d.c000.snappy.parquet', name='part-00003-0365f86b-c7bb-4b19-9577-9035f9483b6d.c000.snappy.parquet', size=15636684, modificationTime=1732631873000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00004-8f2bf93e-a16f-416a-a8b7-2e60491076ca.c000.snappy.parquet', name='part-00004-8f2bf93e-a16f-416a-a8b7-2e60491076ca.c000.snappy.parquet', size=13778857, modificationTime=1732631875000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00005-815f5b0e-ce05-44fd-a33e-7f3bfaf42042.c000.snappy.parquet', name='part-00005-815f5b0e-ce05-44fd-a33e-7f3bfaf42042.c000.snappy.parquet', size=12327484, modificationTime=1732631875000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00006-fcfff104-2451-4c20-8031-334626aaea06.c000.snappy.parquet', name='part-00006-fcfff104-2451-4c20-8031-334626aaea06.c000.snappy.parquet', size=15452402, modificationTime=1732631875000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00007-65fdabbb-4381-48b9-a828-8e47241889b5.c000.snappy.parquet', name='part-00007-65fdabbb-4381-48b9-a828-8e47241889b5.c000.snappy.parquet', size=15645373, modificationTime=1732631875000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00008-77fe0d6f-f200-4050-a030-5b73851c0ae3.c000.snappy.parquet', name='part-00008-77fe0d6f-f200-4050-a030-5b73851c0ae3.c000.snappy.parquet', size=18085753, modificationTime=1732631877000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00009-cdab4e77-3c12-4a31-a765-c5f6a34fc5be.c000.snappy.parquet', name='part-00009-cdab4e77-3c12-4a31-a765-c5f6a34fc5be.c000.snappy.parquet', size=15083243, modificationTime=1732631877000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00010-32af0206-7481-4fe3-b250-c64fc6345241.c000.snappy.parquet', name='part-00010-32af0206-7481-4fe3-b250-c64fc6345241.c000.snappy.parquet', size=17288198, modificationTime=1732631877000),\n",
       " FileInfo(path='dbfs:/dbfs/FileStore/tables/taxi_final_df_cleaned/part-00011-6245181e-747b-4507-9a9a-3aff66a23d79.c000.snappy.parquet', name='part-00011-6245181e-747b-4507-9a9a-3aff66a23d79.c000.snappy.parquet', size=15413837, modificationTime=1732631877000)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/dbfs/FileStore/tables/taxi_final_df_cleaned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c149697d-ad02-4c16-b449-c42272a695d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the cleaned taxi data\n",
    "taxi_df_cleaned = spark.read.format(\"delta\").load(\"/dbfs/FileStore/tables/taxi_final_df_cleaned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754eab15-78ec-4280-b299-0da225ec9e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "taxi_df_cleaned = taxi_df_cleaned.withColumn(\"payment_type\", col(\"payment_type\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50f8b4c-458b-444a-b254-556d7e3fdbbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>passenger_count</th><th>payment_type</th><th>total_amount</th><th>trip_duration</th><th>pickup_day_of_week</th><th>pickup_hour</th><th>pickup_month</th><th>pickup_borough</th><th>dropoff_borough</th><th>is_holiday</th><th>distance_bin</th><th>time_of_day_bin</th><th>near_airport</th></tr></thead><tbody><tr><td>3</td><td>2</td><td>14.7</td><td>9.866666793823242</td><td>2</td><td>9</td><td>10</td><td>Manhattan</td><td>Manhattan</td><td>1</td><td>Short</td><td>Morning</td><td>0</td></tr><tr><td>1</td><td>1</td><td>30.94</td><td>22.616666793823242</td><td>2</td><td>11</td><td>10</td><td>Manhattan</td><td>Manhattan</td><td>1</td><td>Short</td><td>Morning</td><td>0</td></tr><tr><td>1</td><td>1</td><td>26.85</td><td>23.133333206176758</td><td>2</td><td>12</td><td>10</td><td>Manhattan</td><td>Manhattan</td><td>1</td><td>Medium</td><td>Afternoon</td><td>0</td></tr><tr><td>1</td><td>1</td><td>21.88</td><td>12.333333015441895</td><td>2</td><td>13</td><td>10</td><td>Manhattan</td><td>Manhattan</td><td>1</td><td>Short</td><td>Afternoon</td><td>0</td></tr><tr><td>1</td><td>2</td><td>7.7</td><td>1.2999999523162842</td><td>2</td><td>16</td><td>10</td><td>Manhattan</td><td>Manhattan</td><td>1</td><td>Short</td><td>Afternoon</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         2,
         14.7,
         9.866666793823242,
         2,
         9,
         10,
         "Manhattan",
         "Manhattan",
         1,
         "Short",
         "Morning",
         0
        ],
        [
         1,
         1,
         30.94,
         22.616666793823242,
         2,
         11,
         10,
         "Manhattan",
         "Manhattan",
         1,
         "Short",
         "Morning",
         0
        ],
        [
         1,
         1,
         26.85,
         23.133333206176758,
         2,
         12,
         10,
         "Manhattan",
         "Manhattan",
         1,
         "Medium",
         "Afternoon",
         0
        ],
        [
         1,
         1,
         21.88,
         12.333333015441895,
         2,
         13,
         10,
         "Manhattan",
         "Manhattan",
         1,
         "Short",
         "Afternoon",
         0
        ],
        [
         1,
         2,
         7.7,
         1.2999999523162842,
         2,
         16,
         10,
         "Manhattan",
         "Manhattan",
         1,
         "Short",
         "Afternoon",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "payment_type",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "trip_duration",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pickup_day_of_week",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pickup_hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pickup_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pickup_borough",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_borough",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_holiday",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "distance_bin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "time_of_day_bin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "near_airport",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- passenger_count: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- total_amount: float (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- pickup_day_of_week: integer (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- is_holiday: integer (nullable = true)\n |-- distance_bin: string (nullable = true)\n |-- time_of_day_bin: string (nullable = true)\n |-- near_airport: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "display(taxi_df_cleaned.limit(5))\n",
    "taxi_df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bb3e83-c632-48f5-a851-08b832ce3338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data to a smaller fraction to speed up processing\n",
    "sample_data = taxi_df_cleaned.sample(fraction=0.1, seed=42).cache()\n",
    "train_data, test_data = sample_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f4f937-05ea-4226-a9e3-a3dfa3447cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['pickup_borough', 'dropoff_borough', 'distance_bin', 'time_of_day_bin']\n",
    "numerical_cols = ['pickup_day_of_week','payment_type','trip_duration', 'pickup_hour', 'pickup_month','is_holiday','passenger_count', 'near_airport']\n",
    "target_col = \"total_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889c121c-f39a-4c27-8402-11e7edeb3187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## One_Hot_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdbe9a8-e9e1-43d5-bcfe-3ea0d4c70ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define feature transformations\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid=\"error\", dropLast=False) \n",
    "    for col in categorical_cols\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=[col + \"_encoded\" for col in categorical_cols] + numerical_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389dca0b-ae43-4f9e-b509-4d1929bf5d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'linear_reg': LinearRegression(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'decision_tree': DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'random_forest': RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'gradient_boosting': GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe48345-ec51-4682-ab03-1d19066dc212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "r2_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce24ed31-a489-41c3-bf2a-c16c5b4707ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: linear_reg, R²: 0.8290222630088123, MAE: 4.006805069400227\nModel: decision_tree, R²: 0.840119377749878, MAE: 4.15625552005428\nModel: random_forest, R²: 0.8393684764711263, MAE: 3.953835751536056\nModel: gradient_boosting, R²: 0.8666322263220215, MAE: 3.273234513119726\n+-----------------+----------+---------+\n|       model_name|  r2_score|      mae|\n+-----------------+----------+---------+\n|       linear_reg| 0.8290223| 4.006805|\n|    decision_tree|0.84011936|4.1562557|\n|    random_forest|0.83936846|3.9538357|\n|gradient_boosting| 0.8666322|3.2732346|\n+-----------------+----------+---------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, pickup_borough: string, dropoff_borough: string, is_holiday: int, distance_bin: string, time_of_day_bin: string, near_airport: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define K-fold cross-validation function with optimized settings\n",
    "def cross_validate_model(model_name, model, train_data, test_data):\n",
    "    # Pipeline creation\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, model])\n",
    "    \n",
    "    # Set up K-fold cross-validator with 3 folds\n",
    "    param_grid = ParamGridBuilder().build()\n",
    "    cross_val = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=r2_evaluator, numFolds=3, seed=42)\n",
    "    \n",
    "    # Fit cross-validator on training data\n",
    "    cv_model = cross_val.fit(train_data)\n",
    "    \n",
    "    # Get R² score and calculate MAE on the test set\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    r2_score = cv_model.avgMetrics[0]\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    \n",
    "    return (model_name, float(r2_score), float(mae))\n",
    "\n",
    "# Evaluate models sequentially\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    result = cross_validate_model(name, model, train_data, test_data)\n",
    "    results.append(result)\n",
    "    print(f\"Model: {result[0]}, R²: {result[1]}, MAE: {result[2]}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"r2_score\", FloatType(), True),\n",
    "    StructField(\"mae\", FloatType(), True)\n",
    "])\n",
    "results_df = spark.createDataFrame(results, schema=schema)\n",
    "results_df.show()\n",
    "\n",
    "# Unpersist the cached data to free memory\n",
    "sample_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5083cbf1-fa78-4f5a-a2e5-8a9c134aab97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ordinal_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b200f614-867a-4448-b8f3-5a31ebb0ae91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "# Define feature transformations\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=[col + \"_index\" for col in categorical_cols] + numerical_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ce5274-aaef-41f1-a475-e72737361449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'linear_reg': LinearRegression(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'decision_tree': DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'random_forest': RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'gradient_boosting': GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360846c9-6183-483e-be02-d6e02ee3f558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "r2_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155919d3-9c79-4fa6-8b8d-7c3b3f408eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation for model: linear_reg\nModel: linear_reg, R²: 0.81458539050846, MAE: 4.340034381914302\nStarting cross-validation for model: decision_tree\nModel: decision_tree, R²: 0.8393737547555196, MAE: 4.167838610600192\nStarting cross-validation for model: random_forest\nModel: random_forest, R²: 0.8400047138865658, MAE: 4.050909483271984\nStarting cross-validation for model: gradient_boosting\nModel: gradient_boosting, R²: 0.8654929855700676, MAE: 3.315907733149578\n+-----------------+----------+---------+\n|       model_name|  r2_score|      mae|\n+-----------------+----------+---------+\n|       linear_reg| 0.8145854|4.3400345|\n|    decision_tree|0.83937377|4.1678386|\n|    random_forest|0.84000474|4.0509095|\n|gradient_boosting|  0.865493|3.3159077|\n+-----------------+----------+---------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, pickup_borough: string, dropoff_borough: string, is_holiday: int, distance_bin: string, time_of_day_bin: string, near_airport: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Define K-fold cross-validation function\n",
    "def cross_validate_model(model_name, model, train_data, test_data):\n",
    "    # Create pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(stages=indexers + [assembler, scaler, model])\n",
    "    \n",
    "    # Set up 3-fold cross-validation\n",
    "    param_grid = ParamGridBuilder().build()  # No hyperparameter tuning\n",
    "    cross_val = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=r2_evaluator, numFolds=3, seed=42)\n",
    "    \n",
    "    # Fit cross-validator on training data\n",
    "    cv_model = cross_val.fit(train_data)\n",
    "    \n",
    "    # Evaluate model on the test set\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    r2_score = cv_model.avgMetrics[0]\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    \n",
    "    return (model_name, float(r2_score), float(mae))\n",
    "\n",
    "# Evaluate each model and capture results\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"Starting cross-validation for model: {name}\")\n",
    "    result = cross_validate_model(name, model, train_data, test_data)\n",
    "    results.append(result)\n",
    "    print(f\"Model: {result[0]}, R²: {result[1]}, MAE: {result[2]}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"r2_score\", FloatType(), True),\n",
    "    StructField(\"mae\", FloatType(), True)\n",
    "])\n",
    "results_df = spark.createDataFrame(results, schema=schema)\n",
    "results_df.show()\n",
    "\n",
    "# Unpersist the cached data to free memory\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269f8fdc-ee99-49ae-a4f0-02959f1352c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Target_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f06155a-f75f-4f98-a887-4cadee573120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- passenger_count: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- total_amount: float (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- pickup_day_of_week: integer (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- is_holiday: integer (nullable = true)\n |-- distance_bin: string (nullable = true)\n |-- time_of_day_bin: string (nullable = true)\n |-- near_airport: integer (nullable = true)\n\nroot\n |-- passenger_count: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- total_amount: float (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- pickup_day_of_week: integer (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- is_holiday: integer (nullable = true)\n |-- distance_bin: string (nullable = true)\n |-- time_of_day_bin: string (nullable = true)\n |-- near_airport: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51136be0-d438-472f-9355-81dfd00723f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- passenger_count: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- total_amount: float (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- pickup_day_of_week: integer (nullable = true)\n |-- pickup_hour: integer (nullable = true)\n |-- pickup_month: integer (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- is_holiday: integer (nullable = true)\n |-- distance_bin: string (nullable = true)\n |-- time_of_day_bin: string (nullable = true)\n |-- near_airport: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "sample_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c79507-4fd3-4197-a3b1-f2c60b2f439e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[time_of_day_bin: string, distance_bin: string, dropoff_borough: string, pickup_borough: string, passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, is_holiday: int, near_airport: int, pickup_borough_target_mean: double, dropoff_borough_target_mean: double, distance_bin_target_mean: double, time_of_day_bin_target_mean: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# List of categorical columns for target mean encoding\n",
    "categorical_cols = ['pickup_borough', 'dropoff_borough', 'distance_bin', 'time_of_day_bin']\n",
    "\n",
    "# Target column\n",
    "target_col = \"total_amount\"\n",
    "\n",
    "# Dynamically build feature columns list by performing target mean encoding\n",
    "feature_cols = []\n",
    "\n",
    "# Add target mean encoding to train_data and test_data\n",
    "for col in categorical_cols:\n",
    "    # Calculate mean target for each category\n",
    "    category_means = train_data.groupBy(col).agg(F.mean(target_col).alias(f\"{col}_target_mean\"))\n",
    "    \n",
    "    # Join the means to train and test datasets\n",
    "    train_data = train_data.join(category_means, on=col, how=\"left\")\n",
    "    test_data = test_data.join(category_means, on=col, how=\"left\")\n",
    "    \n",
    "    # Add the new encoded column to the list of feature columns\n",
    "    feature_cols.append(f\"{col}_target_mean\")\n",
    "\n",
    "numerical_cols = ['pickup_day_of_week','payment_type','trip_duration', 'pickup_hour', 'pickup_month','is_holiday','passenger_count', 'near_airport']\n",
    "feature_cols.extend(numerical_cols)\n",
    "\n",
    "# Cache the data for efficiency\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a70102-5da5-4d22-af12-94dd1c4f25bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a52cf2d-5e03-4101-a6dd-eb5f7c1b8a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'linear_reg': LinearRegression(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'decision_tree': DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'random_forest': RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col),\n",
    "    'gradient_boosting': GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a93b93-372d-4cc3-9fe5-0d518056c342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "r2_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a5c747-7552-4c9e-9239-e376d8d2b12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation for model: linear_reg\nModel: linear_reg, R²: 0.8254043745319345, MAE: 4.093160624739086\nStarting cross-validation for model: decision_tree\nModel: decision_tree, R²: 0.8401205134193733, MAE: 4.156255520054801\nStarting cross-validation for model: random_forest\nModel: random_forest, R²: 0.8416565814772712, MAE: 4.0155203402921895\nStarting cross-validation for model: gradient_boosting\nModel: gradient_boosting, R²: 0.8667986160797772, MAE: 3.262267981062468\n+-----------------+----------+---------+\n|       model_name|  r2_score|      mae|\n+-----------------+----------+---------+\n|       linear_reg|0.82540435|4.0931606|\n|    decision_tree| 0.8401205|4.1562557|\n|    random_forest|0.84165657|4.0155206|\n|gradient_boosting|0.86679864| 3.262268|\n+-----------------+----------+---------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[time_of_day_bin: string, distance_bin: string, dropoff_borough: string, pickup_borough: string, passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, is_holiday: int, near_airport: int, pickup_borough_target_mean: double, dropoff_borough_target_mean: double, distance_bin_target_mean: double, time_of_day_bin_target_mean: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation function\n",
    "def cross_validate_model(model_name, model, train_data, test_data):\n",
    "    # Create pipeline with assembler, scaler, and model\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, model])\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    param_grid = ParamGridBuilder().build()  # No hyperparameter tuning in this example\n",
    "    cross_val = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=r2_evaluator, numFolds=3, seed=42)\n",
    "    \n",
    "    # Fit cross-validator on training data\n",
    "    cv_model = cross_val.fit(train_data)\n",
    "    \n",
    "    # Evaluate model on test data\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    r2_score = cv_model.avgMetrics[0]\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    \n",
    "    return (model_name, float(r2_score), float(mae))\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"Starting cross-validation for model: {name}\")\n",
    "    result = cross_validate_model(name, model, train_data, test_data)\n",
    "    results.append(result)\n",
    "    print(f\"Model: {result[0]}, R²: {result[1]}, MAE: {result[2]}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"r2_score\", FloatType(), True),\n",
    "    StructField(\"mae\", FloatType(), True)\n",
    "])\n",
    "results_df = spark.createDataFrame(results, schema=schema)\n",
    "results_df.show()\n",
    "\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1095a39e-1af2-4f03-85a4-bf9597a1cb01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## One_Hot_Encoding_with_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea32082-afbf-4fb8-9b8f-f9a605387bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define feature transformations\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid=\"error\", dropLast=False) \n",
    "    for col in categorical_cols\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=[col + \"_encoded\" for col in categorical_cols] + numerical_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1df5dbb-6537-441e-9829-72c2a188d743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\n",
    "\n",
    "# Apply PCA after scaling\n",
    "pca = PCA(k=10, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")  # Adjust k to the desired number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11701f8f-c491-40c6-b765-8a416325d716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define models to evaluate, using `pcaFeatures` as input\n",
    "models = {\n",
    "    'linear_reg': LinearRegression(featuresCol=\"pcaFeatures\", labelCol=target_col),\n",
    "    'decision_tree': DecisionTreeRegressor(featuresCol=\"pcaFeatures\", labelCol=target_col),\n",
    "    'random_forest': RandomForestRegressor(featuresCol=\"pcaFeatures\", labelCol=target_col),\n",
    "    'gradient_boosting': GBTRegressor(featuresCol=\"pcaFeatures\", labelCol=target_col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d2d93a-998e-4efe-942b-10b83099d516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "r2_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db6aa2e-7756-4765-b2a5-009e9d4f4531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation for model: linear_reg\nModel: linear_reg, R²: 0.7267091456133968, MAE: 5.4649227481979885\nStarting cross-validation for model: decision_tree\nModel: decision_tree, R²: 0.7985795569206094, MAE: 4.454893242366173\nStarting cross-validation for model: random_forest\nModel: random_forest, R²: 0.8015923178604, MAE: 4.475081132878776\nStarting cross-validation for model: gradient_boosting\nModel: gradient_boosting, R²: 0.841954093304136, MAE: 3.7819059343806676\n+-----------------+---------+--------+\n|       model_name| r2_score|     mae|\n+-----------------+---------+--------+\n|       linear_reg|0.7267091|5.464923|\n|    decision_tree|0.7985796|4.454893|\n|    random_forest|0.8015923|4.475081|\n|gradient_boosting|0.8419541|3.781906|\n+-----------------+---------+--------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[time_of_day_bin: string, distance_bin: string, dropoff_borough: string, pickup_borough: string, passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, is_holiday: int, near_airport: int, pickup_borough_target_mean: double, dropoff_borough_target_mean: double, distance_bin_target_mean: double, time_of_day_bin_target_mean: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define K-fold cross-validation function\n",
    "def cross_validate_model(model_name, model, train_data, test_data):\n",
    "    # Pipeline creation with One-Hot Encoding, PCA, and model\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, pca, model])\n",
    "    \n",
    "    # Set up 3-fold cross-validation\n",
    "    param_grid = ParamGridBuilder().build()  # No hyperparameter tuning\n",
    "    cross_val = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=r2_evaluator, numFolds=3, seed=42)\n",
    "    \n",
    "    # Fit cross-validator on training data\n",
    "    cv_model = cross_val.fit(train_data)\n",
    "    \n",
    "    # Evaluate model on the test set\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    r2_score = cv_model.avgMetrics[0]\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    \n",
    "    return (model_name, float(r2_score), float(mae))\n",
    "\n",
    "# Evaluate each model and capture results\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"Starting cross-validation for model: {name}\")\n",
    "    result = cross_validate_model(name, model, train_data, test_data)\n",
    "    results.append(result)\n",
    "    print(f\"Model: {result[0]}, R²: {result[1]}, MAE: {result[2]}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"r2_score\", FloatType(), True),\n",
    "    StructField(\"mae\", FloatType(), True)\n",
    "])\n",
    "results_df = spark.createDataFrame(results, schema=schema)\n",
    "results_df.show()\n",
    "\n",
    "# Unpersist the cached data to free memory\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "544e179c-aca9-4c58-bd1b-6ca7de8c413b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa447a2-c1b8-475d-b31b-eb5804ecdb8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6337b8ae-a4e9-49f4-adc5-3319627cb354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Linear Regression...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define the Linear Regression model\n",
    "linear_reg_model = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "\n",
    "# For Linear Regression\n",
    "print(\"Processing Linear Regression...\")\n",
    "linear_reg_pipeline = Pipeline(stages=indexers + encoders + \n",
    "                               [assembler.setOutputCol(\"features\"),\n",
    "                                scaler.setInputCol(\"features\").setOutputCol(\"scaledFeatures\"), \n",
    "                                linear_reg_model])\n",
    "\n",
    "# Define parameter grid for Linear Regression\n",
    "param_grid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(linear_reg_model.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(linear_reg_model.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=linear_reg_pipeline,\n",
    "    estimatorParamMaps=param_grid_lr,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Fit the cross-validation model\n",
    "cv_lr_model = cv_lr.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "lr_best_model = cv_lr_model.bestModel\n",
    "\n",
    "# Make predictions on the test data\n",
    "lr_predictions = lr_best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# Append results\n",
    "results.append((\"Linear Regression\", lr_r2, lr_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c6c9fc-30bf-4939-a5c6-3fd80c904402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linear Regression regParam: 0.001\nBest Linear Regression elasticNetParam: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Access the best Linear Regression model from the pipeline\n",
    "best_lr_model = lr_best_model.stages[-1]\n",
    "\n",
    "# Retrieve the best parameters for Linear Regression\n",
    "best_reg_param_lr = best_lr_model.getRegParam()\n",
    "best_elastic_net_param_lr = best_lr_model.getElasticNetParam()\n",
    "\n",
    "# Print the best Linear Regression parameters\n",
    "print(f\"Best Linear Regression regParam: {best_reg_param_lr}\")\n",
    "print(f\"Best Linear Regression elasticNetParam: {best_elastic_net_param_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c083547c-17ef-418d-9a00-e68115509715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Decision Tree...\nDecision Tree - R²: 0.8744, MAE: 3.0984\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[time_of_day_bin: string, distance_bin: string, dropoff_borough: string, pickup_borough: string, passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, is_holiday: int, near_airport: int, pickup_borough_target_mean: double, dropoff_borough_target_mean: double, distance_bin_target_mean: double, time_of_day_bin_target_mean: double]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Define the Decision Tree model\n",
    "decision_tree_model = DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "\n",
    "# For Decision Tree\n",
    "print(\"Processing Decision Tree...\")\n",
    "\n",
    "# Create a pipeline specifically for Decision Tree without PCA\n",
    "decision_tree_pipeline = Pipeline(stages=indexers + encoders + [\n",
    "    assembler.setOutputCol(\"features\"),\n",
    "    scaler.setInputCol(\"features\").setOutputCol(\"scaledFeatures\"),\n",
    "    decision_tree_model\n",
    "])\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "param_grid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree_model.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(decision_tree_model.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up CrossValidator for Decision Tree\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=decision_tree_pipeline,\n",
    "    estimatorParamMaps=param_grid_dt,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model using CrossValidator\n",
    "cv_dt_model = cv_dt.fit(train_data)\n",
    "\n",
    "# Retrieve the best model from cross-validation\n",
    "dt_best_model = cv_dt_model.bestModel\n",
    "\n",
    "# Make predictions on the test data\n",
    "dt_predictions = dt_best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using R² and MAE\n",
    "dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "dt_mae = mae_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "# Append results to the list\n",
    "results.append((\"Decision Tree\", dt_r2, dt_mae))\n",
    "\n",
    "# Print the results for the Decision Tree model\n",
    "print(f\"Decision Tree - R²: {dt_r2:.4f}, MAE: {dt_mae:.4f}\")\n",
    "\n",
    "# Unpersist the data after use\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d520c8f-d358-4d7d-8abe-40981f19640f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best maxDepth: 10\nBest maxBins: 64\n"
     ]
    }
   ],
   "source": [
    "# Access the best hyperparameters from the cross-validation\n",
    "best_max_depth = dt_best_model.stages[-1]._java_obj.getMaxDepth()\n",
    "best_max_bins = dt_best_model.stages[-1]._java_obj.getMaxBins()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best maxDepth: {best_max_depth}\")\n",
    "print(f\"Best maxBins: {best_max_bins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f3945a-1899-4191-9705-a31d313cce6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Random Forest Regressor...\nRandom Forest Regressor - R²: 0.8760, MAE: 3.1299\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, pickup_borough: string, dropoff_borough: string, is_holiday: int, distance_bin: string, time_of_day_bin: string, near_airport: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the Random Forest Regressor model\n",
    "random_forest_model = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col, seed=42)\n",
    "\n",
    "print(\"Processing Random Forest Regressor...\")\n",
    "\n",
    "# Create a pipeline specifically for Random Forest without PCA\n",
    "random_forest_pipeline = Pipeline(stages=indexers + encoders + [\n",
    "    assembler.setOutputCol(\"features\"),\n",
    "    scaler.setInputCol(\"features\").setOutputCol(\"scaledFeatures\"),\n",
    "    random_forest_model \n",
    "])\n",
    "\n",
    "# Define a simplified parameter grid for Random Forest\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest_model.maxDepth, [5, 10]) \\\n",
    "    .addGrid(random_forest_model.numTrees, [10, 25]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up TrainValidationSplit\n",
    "train_val_rf = TrainValidationSplit(\n",
    "    estimator=random_forest_pipeline,\n",
    "    estimatorParamMaps=param_grid_rf,\n",
    "    evaluator=r2_evaluator,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model using TrainValidationSplit\n",
    "cv_rf_model = train_val_rf.fit(train_data)\n",
    "\n",
    "# Retrieve the best model from TrainValidationSplit\n",
    "rf_best_model = cv_rf_model.bestModel\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_predictions = rf_best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using R² and MAE\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# Print the results for the Random Forest Regressor model\n",
    "print(f\"Random Forest Regressor - R²: {rf_r2:.4f}, MAE: {rf_mae:.4f}\")\n",
    "\n",
    "# Append results to the list\n",
    "results.append((\"Random Forest Regressor\", rf_r2, rf_mae))\n",
    "\n",
    "# Unpersist the data after use\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6700c789-a387-4d69-8611-60a78cb1e176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved locally at /dbfs/tmp/random_forest_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the model save path in Databricks' file system\n",
    "local_path = \"/dbfs/tmp/random_forest_model\"\n",
    "\n",
    "# Save the best Random Forest model\n",
    "rf_best_model.write().overwrite().save(local_path)\n",
    "\n",
    "print(f\"Model saved locally at {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bde7152-8ece-4604-a160-95744aed0125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data:\n+---------------+------------+-------------+------------------+-----------+------------+--------------+---------------+----------+------------+---------------+------------+\n|passenger_count|payment_type|trip_duration|pickup_day_of_week|pickup_hour|pickup_month|pickup_borough|dropoff_borough|is_holiday|distance_bin|time_of_day_bin|near_airport|\n+---------------+------------+-------------+------------------+-----------+------------+--------------+---------------+----------+------------+---------------+------------+\n|              1|           1|        20.37|                 2|          8|          10|     Manhattan|       Brooklyn|         0|      Medium|        Morning|           0|\n+---------------+------------+-------------+------------------+-----------+------------+--------------+---------------+----------+------------+---------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Example single query input data\n",
    "single_query = [\n",
    "    (1, 1, 20.37, 2, 8, 10, \"Manhattan\", \"Brooklyn\", 0, \"Medium\", \"Morning\", 0)\n",
    "]\n",
    "\n",
    "# Define the schema of your input data based on the model's feature columns\n",
    "columns = [\n",
    "    \"passenger_count\", \"payment_type\", \"trip_duration\", \n",
    "    \"pickup_day_of_week\", \"pickup_hour\", \"pickup_month\", \"pickup_borough\", \n",
    "    \"dropoff_borough\", \"is_holiday\", \"distance_bin\", \"time_of_day_bin\", \"near_airport\"\n",
    "]\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ModelPrediction\").getOrCreate()\n",
    "\n",
    "# Convert the input query to a DataFrame\n",
    "test_data_df = spark.createDataFrame(single_query, columns)\n",
    "\n",
    "# Show the test data to verify the schema\n",
    "print(\"Test Data:\")\n",
    "test_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e1fee81-e2cb-4c07-a676-3f3119d67ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the best Random Forest model from the pipeline\n",
    "best_rf_model = rf_best_model.stages[-1]\n",
    "\n",
    "# Retrieve the best parameters for Random Forest\n",
    "best_max_depth_rf = best_rf_model.getMaxDepth()  # Correct usage: No parentheses\n",
    "best_num_trees_rf = best_rf_model.getNumTrees\n",
    "\n",
    "# Print the best Random Forest parameters\n",
    "print(f\"Best Random Forest maxDepth: {best_max_depth_rf}\")\n",
    "print(f\"Best Random Forest numTrees: {best_num_trees_rf}\")\n",
    "print(f\"Best Random Forest: {best_rf_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b43eb61-ef50-463b-ac33-fd7c0eea5e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gradient Boosted Tree Regressor...\nGradient Boosted Tree Regressor - R²: 0.8723, MAE: 3.2086\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, payment_type: int, total_amount: float, trip_duration: double, pickup_day_of_week: int, pickup_hour: int, pickup_month: int, pickup_borough: string, dropoff_borough: string, is_holiday: int, distance_bin: string, time_of_day_bin: string, near_airport: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the Gradient Boosted Tree Regressor model\n",
    "gradient_boosting_model = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col, seed=42)\n",
    "\n",
    "print(\"Processing Gradient Boosted Tree Regressor...\")\n",
    "\n",
    "# Cache train and test data\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "# Create a pipeline specifically for GBT\n",
    "gradient_boosting_pipeline = Pipeline(stages=indexers + encoders + [\n",
    "    assembler.setOutputCol(\"features\"),\n",
    "    scaler.setInputCol(\"features\").setOutputCol(\"scaledFeatures\"),\n",
    "    gradient_boosting_model\n",
    "])\n",
    "\n",
    "# Define a simplified parameter grid for GBT\n",
    "param_grid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gradient_boosting_model.maxIter, [10, 25]) \\\n",
    "    .addGrid(gradient_boosting_model.maxDepth, [3, 5]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up TrainValidationSplit\n",
    "train_val_gbt = TrainValidationSplit(\n",
    "    estimator=gradient_boosting_pipeline,\n",
    "    estimatorParamMaps=param_grid_gbt,\n",
    "    evaluator=r2_evaluator,  # R² evaluator\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model using TrainValidationSplit\n",
    "cv_gbt_model = train_val_gbt.fit(train_data)\n",
    "\n",
    "# Retrieve the best model from TrainValidationSplit\n",
    "gbt_best_model = cv_gbt_model.bestModel\n",
    "\n",
    "# Make predictions on the test data\n",
    "gbt_predictions = gbt_best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using R² and MAE\n",
    "gbt_r2 = r2_evaluator.evaluate(gbt_predictions)\n",
    "gbt_mae = mae_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Print the results for the Gradient Boosted Tree Regressor model\n",
    "print(f\"Gradient Boosted Tree Regressor - R²: {gbt_r2:.4f}, MAE: {gbt_mae:.4f}\")\n",
    "\n",
    "# Append results to the list (optional, if you need to store the results)\n",
    "results.append((\"Gradient Boosted Tree Regressor\", gbt_r2, gbt_mae))\n",
    "\n",
    "# Unpersist the data after use\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29caf421-8265-4cc7-b7a4-3bc2ceb3e23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the best Gradient Boosted Tree model from the pipeline\n",
    "best_gbt_model = cv_gbt_model.bestModel.stages[-1]\n",
    "\n",
    "# Retrieve the best parameters for Gradient Boosted Tree\n",
    "best_max_iter_gbt = best_gbt_model.getMaxIter()\n",
    "best_max_depth_gbt = best_gbt_model.getMaxDepth()\n",
    "\n",
    "# Print the best Gradient Boosted Tree parameters\n",
    "print(f\"Best Gradient Boosted Tree maxIter: {best_max_iter_gbt}\")\n",
    "print(f\"Best Gradient Boosted Tree maxDepth: {best_max_depth_gbt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f5c8ba-080b-41c7-a255-a960e74e564a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Create a dictionary to store the best parameters for each model\n",
    "best_params = {\n",
    "    \"Linear Regression\": {\n",
    "        \"regParam\": best_reg_param_lr,\n",
    "        \"elasticNetParam\": best_elastic_net_param_lr\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"maxDepth\": best_max_depth,\n",
    "        \"maxBins\": best_max_bins\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"maxDepth\": best_max_depth_rf,\n",
    "        \"numTrees\": best_num_trees_rf\n",
    "    },\n",
    "    \"Gradient Boosted Tree\": {\n",
    "        \"maxIter\": best_max_iter_gbt,\n",
    "        \"maxDepth\": best_max_depth_gbt\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store the evaluation results in a structured format\n",
    "results = [\n",
    "    Row(model_name=\"Linear Regression\", hyperparameters=best_params[\"Linear Regression\"], r2_score=lr_r2, mae=lr_mae),\n",
    "    Row(model_name=\"Decision Tree\", hyperparameters=best_params[\"Decision Tree\"], r2_score=dt_r2, mae=dt_mae),\n",
    "    Row(model_name=\"Random Forest\", hyperparameters=best_params[\"Random Forest\"], r2_score=rf_r2, mae=rf_mae),\n",
    "    Row(model_name=\"Gradient Boosted Tree\", hyperparameters=best_params[\"Gradient Boosted Tree\"], r2_score=gbt_r2, mae=gbt_mae)\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"hyperparameters\", StringType(), True),\n",
    "    StructField(\"r2_score\", FloatType(), True),\n",
    "    StructField(\"mae\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Convert the results list into a DataFrame\n",
    "results_df = spark.createDataFrame(results, schema=schema)\n",
    "\n",
    "# Show the results DataFrame\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Unpersist cached data\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de36554d-caed-4bc7-b2e6-c303670ab37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dictionary of models and their evaluation metrics\n",
    "models_results = {\n",
    "    \"Linear Regression\": {\"r2\": lr_r2, \"mae\": lr_mae},\n",
    "    \"Decision Tree\": {\"r2\": dt_r2, \"mae\": dt_mae},\n",
    "    \"Random Forest\": {\"r2\": rf_r2, \"mae\": rf_mae},\n",
    "    \"Gradient Boosted Tree\": {\"r2\": gbt_r2, \"mae\": gbt_mae}\n",
    "}\n",
    "\n",
    "# Extract model names, R² scores, and MAE scores from the dictionary\n",
    "models = list(models_results.keys())\n",
    "r2_scores = [models_results[model][\"r2\"] for model in models]\n",
    "mae_scores = [models_results[model][\"mae\"] for model in models]\n",
    "\n",
    "# Create a figure with subplots (side-by-side layout)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot R² scores\n",
    "ax[0].bar(models, r2_scores, color='green')\n",
    "ax[0].set_title(\"R² Scores\")\n",
    "ax[0].set_ylabel(\"R²\")\n",
    "ax[0].grid(True, axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better readability\n",
    "\n",
    "# Plot MAE scores\n",
    "ax[1].bar(models, mae_scores, color='red')\n",
    "ax[1].set_title(\"MAE Scores\")\n",
    "ax[1].set_ylabel(\"MAE\")\n",
    "ax[1].grid(True, axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better readability\n",
    "\n",
    "# Adjust layout to prevent overlap and improve spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11_Model_Selection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
